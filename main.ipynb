{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 文本智能校对大赛初赛Baseline\n",
    "\n",
    ">>[赛事链接：文本智能校对大赛](https://aistudio.baidu.com/aistudio/competition/detail/404/0/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、数据集介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 368228\r\n",
      "-rw-r--r-- 1 aistudio aistudio    214928 Jul 15 15:14 preliminary_a_test_source.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio    400324 Jul 15 15:14 preliminary_extend_train.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio 376015154 Jul 15 15:14 preliminary_train.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio    423209 Jul 15 15:14 preliminary_val.json\r\n",
      "-rw-r--r-- 1 aistudio aistudio      1381 Jul 15 15:14 README.md\r\n"
     ]
    }
   ],
   "source": [
    "# 遍历数据文件夹\n",
    "!ls -l /home/aistudio/all_data/preliminary_a_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- preliminary_train：伪数据约100w, 均为负样本\n",
    "- preliminary_extend_train: 真实场景训练数据约1000条, 均为负样本\n",
    "- preliminary_val：真实场景下验证集约1000条(包括约500条正样本和500条负样本）\n",
    "- preliminary_a_test_source: 真实场景下测试集约1000条（包括约500条正样本和500条负样本）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、思路\n",
    "\n",
    "我们可以把数据分为正确数据（即不需要纠错），错别字，语义错误，错别字+语义错误4类。\n",
    "\n",
    "简单来看可以把纠错后与纠错前等长的视为错别字，不等长的视为语义错误；而两者混合错误不好判断，暂不考虑。\n",
    "\n",
    "但是按上面做法可以发现：\n",
    "1. 字数相同的也会出现语义错误，这种是颠倒语序造成的语义错误\n",
    "2. 语义错误很多是字词重复\n",
    "\n",
    "# 三、做法\n",
    "\n",
    "1. 分类数据\n",
    "2. 对识别为错字的先进行错字纠错；无变化再进行语义错误纠错\n",
    "3. 语义错误纠错包含两部分：\n",
    "\n",
    "\t3.1 算法识别重复内容去除\n",
    "\n",
    "\t3.2 语义纠错模型纠错\n",
    "    \n",
    "**本文作者训练的模型已上传到数据集中，[文本智能校对预训练模型](https://aistudio.baidu.com/aistudio/datasetdetail/158748)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、训练分类模型\n",
    "\n",
    "* 在nezha模型、ernie模型或其他模型的基础上，首先使用preliminary_train作为训练集，preliminary_extend_train作为验证集训练模型，该阶段模型保存在*_ckpt文件夹中\n",
    "\n",
    "* 再在训练后模型基础上，使用preliminary_extend_train作为训练集，preliminary_val作为验证集训练模型，该阶段模型保存在*_ft_ckpt文件夹中\n",
    "\n",
    "* 最后在上一步模型基础上，使用preliminary_val作为训练集训练模型，该阶段模型保存在*_el_ckpt文件夹中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python train_classification.py --model_name_or_path nezha_el_ckpt --learning_rate 2e-5\\\r\n",
    " --train_data_path all_data/preliminary_a_data/preliminary_val.json --train_data_is_ground_eval True\\\r\n",
    " --eval_data_path all_data/preliminary_a_data/preliminary_val.json --eval_data_is_ground_eval True\\\r\n",
    " --max_seq_length 128 --batch_size 32\\\r\n",
    " --model_save_path nezha_el_ckpt --epoch 30 --print_step 10 --eval_step 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 预测测试集的type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\r\n",
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\r\n",
    "from eval import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-07-19 00:50:06,729] [    INFO] - We are using <class 'paddlenlp.transformers.nezha.modeling.NeZhaForSequenceClassification'> to load 'nezha_el_ckpt'.\n",
      "[2022-07-19 00:50:11,175] [    INFO] - We are using <class 'paddlenlp.transformers.nezha.tokenizer.NeZhaTokenizer'> to load 'nezha_el_ckpt'.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"nezha_el_ckpt\"\r\n",
    "num_classes = 3\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=num_classes)\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resule_save = []\r\n",
    "with open('all_data/preliminary_a_data/preliminary_a_test_source.json') as f:\r\n",
    "    test_raw_data = json.load(f)\r\n",
    "test_data = [{'text': data['source']} for data in test_raw_data]\r\n",
    "label_map = {0: 'Positive', 1: 'Misspelled words', 2: 'Semantic Error'}\r\n",
    "\r\n",
    "results = predict(model, test_data, tokenizer, label_map, batch_size=32)\r\n",
    "for idx, text in enumerate(test_data):\r\n",
    "    resule_save.append({'source': text['text'], 'type': results[idx], 'id': test_raw_data[idx]['id']})\r\n",
    "    print('Data: {} \\t Lable: {}'.format(text, results[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019\n"
     ]
    }
   ],
   "source": [
    "print(len(resule_save))\r\n",
    "with open('preliminary_a_test_source_with_type.json', 'w') as f:\r\n",
    "    json.dump(resule_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、分离两种错误数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misspell_words_data = ''\r\n",
    "semantic_error_data = ''\r\n",
    "\r\n",
    "with open('all_data/preliminary_a_data/preliminary_train.json') as f:\r\n",
    "    all_data = json.load(f)\r\n",
    "    for data in all_data:\r\n",
    "        if len(data['source']) == len(data['target']):\r\n",
    "            misspell_words_data += data['source'] + '\\t' + data['target'] + '\\n'\r\n",
    "        else:\r\n",
    "            semantic_error_data += data['source'] + '\\t' + data['target'] + '\\n'\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('all_data/preliminary_a_misspelled_words_data/preliminary_train.txt', 'w') as f:\r\n",
    "    f.write(misspell_words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('all_data/preliminary_a_semantic_error_data/preliminary_train.txt', 'w') as f:\r\n",
    "    f.write(semantic_error_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misspell_words_data = ''\r\n",
    "semantic_error_data = ''\r\n",
    "\r\n",
    "with open('all_data/preliminary_a_data/preliminary_extend_train.json') as f:\r\n",
    "    all_data = json.load(f)\r\n",
    "    for data in all_data:\r\n",
    "        if len(data['source']) == len(data['target']):\r\n",
    "            misspell_words_data += data['source'] + '\\t' + data['target'] + '\\n'\r\n",
    "        else:\r\n",
    "            semantic_error_data += data['source'] + '\\t' + data['target'] + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('all_data/preliminary_a_misspelled_words_data/preliminary_extend_train.txt', 'w') as f:\r\n",
    "    f.write(misspell_words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('all_data/preliminary_a_semantic_error_data/preliminary_extend_train.txt', 'w') as f:\r\n",
    "    f.write(semantic_error_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "misspell_words_data = ''\r\n",
    "semantic_error_data = ''\r\n",
    "\r\n",
    "with open('all_data/preliminary_a_data/preliminary_val.json') as f:\r\n",
    "    all_data = json.load(f)\r\n",
    "    for data in all_data:\r\n",
    "        if data['type'] == 'negative':\r\n",
    "            if len(data['source']) == len(data['target']):\r\n",
    "                misspell_words_data += data['source'] + '\\t' + data['target'] + '\\n'\r\n",
    "            else:\r\n",
    "                semantic_error_data += data['source'] + '\\t' + data['target'] + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('all_data/preliminary_a_misspelled_words_data/preliminary_val.txt', 'w') as f:\r\n",
    "    f.write(misspell_words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('all_data/preliminary_a_semantic_error_data/preliminary_val.txt', 'w') as f:\r\n",
    "    f.write(semantic_error_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 五、ERNIE-CSC模型 纠正错字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/ernie_csc\n"
     ]
    }
   ],
   "source": [
    "%cd ernie_csc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 22934/22934 [17:17<00:00, 22.10it/s]\n"
     ]
    }
   ],
   "source": [
    "! python download.py --data_dir ./extra_train_ds/ --url https://github.com/wdimmy/Automatic-Corpus-Generation/raw/master/corpus/train.sgml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python change_sgml_to_txt.py -i extra_train_ds/train.sgml -o extra_train_ds/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python train.py --batch_size 40 --logging_steps 100 --epochs 5 --learning_rate 5e-5 --max_seq_length 128\\\r\n",
    " --model_name_or_path ernie-3.0-xbase-zh\\\r\n",
    " --full_model_path ../ernie_csc_pre_ckpt/best_model.pdparams\\\r\n",
    " --output_dir ../ernie_csc_ft_ckpt/ --extra_train_ds_dir ./extra_train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 导出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-07-17 16:34:19,931] [    INFO]\u001b[0m - Already cached /home/aistudio/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh.pdparams\u001b[0m\n",
      "W0717 16:34:19.933907 77787 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 11.2, Runtime API Version: 11.2\n",
      "W0717 16:34:19.937021 77787 gpu_context.cc:306] device: 0, cuDNN Version: 8.2.\n",
      "\u001b[32m[2022-07-17 16:34:26,181] [    INFO]\u001b[0m - Weights from pretrained model not used in ErnieModel: ['cls.predictions.layer_norm.bias', 'cls.predictions.transform.weight', 'cls.predictions.decoder_bias', 'cls.predictions.transform.bias', 'cls.predictions.layer_norm.weight']\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python export_model.py --model_name_or_path ernie-3.0-xbase-zh --params_path ../ernie_csc_pre_ckpt/best_model.pdparams --output_path ../ernie_csc_infer_model/static_graph_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-07-19 00:04:04,168] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt\n",
      "[2022-07-19 00:04:04,195] [    INFO] - tokenizer config file saved in /home/aistudio/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json\n",
      "[2022-07-19 00:04:04,197] [    INFO] - Special tokens file saved in /home/aistudio/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: 遇到逆竟时，我们必须勇于面对，而且要愈挫愈勇，这样我们才能朝著成功之路前进。\n",
      "Target: 遇到逆境时，我们必须勇于面对，而且要愈挫愈勇，这样我们才能朝著成功之路前进。\n",
      "Source: 人生就是如此，经过磨练才能让自己更加拙壮，才能使自己更加乐观。\n",
      "Target: 人生就是如此，经过磨练才能让自己更加茁壮，才能使自己更加乐观。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[35m--- Running analysis [ir_graph_build_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_graph_clean_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_analysis_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [is_test_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [simplify_with_basic_ops_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]\u001b[0m\n",
      "I0719 00:04:05.387586 39570 fuse_pass_base.cc:57] ---  detected 1 subgraphs\n",
      "\u001b[32m--- Running IR pass [multihead_matmul_fuse_pass_v2]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass]\u001b[0m\n",
      "I0719 00:04:05.718148 39570 fuse_pass_base.cc:57] ---  detected 242 subgraphs\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass]\u001b[0m\n",
      "I0719 00:04:05.746395 39570 fuse_pass_base.cc:57] ---  detected 80 subgraphs\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [fc_fuse_pass]\u001b[0m\n",
      "I0719 00:04:06.649116 39570 fuse_pass_base.cc:57] ---  detected 242 subgraphs\n",
      "\u001b[32m--- Running IR pass [fc_elementwise_layernorm_fuse_pass]\u001b[0m\n",
      "I0719 00:04:06.909458 39570 fuse_pass_base.cc:57] ---  detected 80 subgraphs\n",
      "\u001b[32m--- Running IR pass [conv_elementwise_add_act_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_elementwise_add2_act_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_elementwise_add_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [transpose_flatten_concat_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [runtime_context_cache_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_params_sync_among_devices_pass]\u001b[0m\n",
      "I0719 00:04:07.052198 39570 ir_params_sync_among_devices_pass.cc:100] Sync params from CPU to GPU\n",
      "\u001b[1m\u001b[35m--- Running analysis [adjust_cudnn_workspace_size_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [inference_op_replace_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_graph_to_program_pass]\u001b[0m\n",
      "I0719 00:04:08.071785 39570 analysis_predictor.cc:1007] ======= optimize end =======\n",
      "I0719 00:04:08.097136 39570 naive_executor.cc:102] ---  skip [feed], feed -> pinyin_ids\n",
      "I0719 00:04:08.097163 39570 naive_executor.cc:102] ---  skip [feed], feed -> input_ids\n",
      "I0719 00:04:08.106650 39570 naive_executor.cc:102] ---  skip [softmax_21.tmp_0], fetch -> fetch\n",
      "I0719 00:04:08.106679 39570 naive_executor.cc:102] ---  skip [linear_364.tmp_1], fetch -> fetch\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import ErnieTokenizer\r\n",
    "from predict import Predictor\r\n",
    "from paddlenlp.data import Vocab\r\n",
    "\r\n",
    "tokenizer = ErnieTokenizer.from_pretrained('ernie-3.0-xbase-zh')\r\n",
    "pinyin_vocab = Vocab.load_vocabulary('./pinyin_vocab.txt',\r\n",
    "                                     unk_token='[UNK]',\r\n",
    "                                     pad_token='[PAD]')\r\n",
    "predictor = Predictor('../ernie_csc_infer_model/static_graph_params.pdmodel',\r\n",
    "                      '../ernie_csc_infer_model/static_graph_params.pdiparams', \r\n",
    "                      'gpu', 128, tokenizer, pinyin_vocab)\r\n",
    "\r\n",
    "samples = [\r\n",
    "    '遇到逆竟时，我们必须勇于面对，而且要愈挫愈勇，这样我们才能朝著成功之路前进。',\r\n",
    "    '人生就是如此，经过磨练才能让自己更加拙壮，才能使自己更加乐观。',\r\n",
    "]\r\n",
    "\r\n",
    "results = predictor.predict(samples, batch_size=2)\r\n",
    "for source, target in zip(samples, results):\r\n",
    "    print(\"Source:\", source)\r\n",
    "    print(\"Target:\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 六、T5模型 纠正语法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/t5\n"
     ]
    }
   ],
   "source": [
    "%cd t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python train.py --batch_size 32 --logging_steps 100 --epochs 5 --learning_rate 5e-5 --max_seq_length 128\\\r\n",
    " --model_name_or_path Langboat/mengzi-t5-base\\\r\n",
    " --output_dir ../t5_ft_ckpt/ --train_ds_dir ../all_data/preliminary_a_semantic_error_data --eval_ds_dir ../all_data/preliminary_a_semantic_error_data/eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "from paddlenlp.transformers import T5ForConditionalGeneration, T5Tokenizer\r\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-07-18 20:03:57,432] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/Langboat/mengzi-t5-base/spiece.model\n",
      "[2022-07-18 20:03:57,434] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/community/Langboat/mengzi-t5-base/added_tokens.json and saved to /home/aistudio/.paddlenlp/models/Langboat/mengzi-t5-base\n",
      "[2022-07-18 20:03:57,436] [    INFO] - Downloading added_tokens.json from https://bj.bcebos.com/paddlenlp/models/community/Langboat/mengzi-t5-base/added_tokens.json\n",
      "[2022-07-18 20:03:57,471] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/community/Langboat/mengzi-t5-base/special_tokens_map.json and saved to /home/aistudio/.paddlenlp/models/Langboat/mengzi-t5-base\n",
      "[2022-07-18 20:03:57,474] [    INFO] - Downloading special_tokens_map.json from https://bj.bcebos.com/paddlenlp/models/community/Langboat/mengzi-t5-base/special_tokens_map.json\n",
      "[2022-07-18 20:03:57,517] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/Langboat/mengzi-t5-base/tokenizer_config.json\n",
      "W0718 20:03:57.622402 39570 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0718 20:03:57.625485 39570 gpu_context.cc:306] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('Langboat/mengzi-t5-base')\r\n",
    "model = T5ForConditionalGeneration.from_pretrained('../t5_ft_ckpt/model_38000_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_t5_pre = []\r\n",
    "with open('../all_data/preliminary_a_data/preliminary_a_test_source.json') as f:\r\n",
    "    all_data = json.load(f)\r\n",
    "    for data in all_data:\r\n",
    "        text = data['source']\r\n",
    "        inputs = tokenizer(text)\r\n",
    "        inputs = {k:paddle.to_tensor([v]) for (k, v) in inputs.items()}\r\n",
    "        output = model.generate(**inputs)\r\n",
    "        gen_text = tokenizer.decode(list(output[0].numpy()[0]), skip_special_tokens=True)\r\n",
    "        gen_text = gen_text.replace(',', '，')\r\n",
    "        gen_text.replace(gen_text[-3:], text[text.rfind(gen_text[-3:]):])\r\n",
    "        gen_text = gen_text if text.rfind(gen_text[-3:]) == -1 else gen_text + text[text.rfind(gen_text[-3:])+3:]\r\n",
    "        res_t5_pre.append({'inference': gen_text, 'id': data['id']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019\n"
     ]
    }
   ],
   "source": [
    "print(len(res_t5_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../preliminary_a_test_inference.json', 'w') as f:\r\n",
    "    json.dump(res_t5_pre, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 七、去重\n",
    "\n",
    "不考虑效率就用了比较慢的循环方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_duplication(text: str):\r\n",
    "    length = len(text)\r\n",
    "    for i in range(length):\r\n",
    "        cp_range = min(i+1, length-i-1)\r\n",
    "        j = 0\r\n",
    "        flag = False\r\n",
    "        for j in range(cp_range):\r\n",
    "            if text[i-j:i+1] == text[i+1:i+1+j+1]:\r\n",
    "                flag = True\r\n",
    "                break\r\n",
    "        if flag:\r\n",
    "            return text.replace(text[i-j:i+1], '', 1)\r\n",
    "\r\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我爱你'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_duplication('我爱爱你')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 八、综合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 首先使路径在 '/home/aistudio'，然后运行分类模型，对测试数据进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-07-19 00:50:21,887] [    INFO] - We are using <class 'paddlenlp.transformers.nezha.modeling.NeZhaForSequenceClassification'> to load 'nezha_el_ckpt'.\n",
      "[2022-07-19 00:50:26,258] [    INFO] - We are using <class 'paddlenlp.transformers.nezha.tokenizer.NeZhaTokenizer'> to load 'nezha_el_ckpt'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019\r"
     ]
    }
   ],
   "source": [
    "import json\r\n",
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\r\n",
    "from eval import predict\r\n",
    "\r\n",
    "model_name = \"nezha_el_ckpt\"\r\n",
    "num_classes = 3\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=num_classes)\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n",
    "\r\n",
    "resule_save = []\r\n",
    "with open('all_data/preliminary_a_data/preliminary_a_test_source.json') as f:\r\n",
    "    test_raw_data = json.load(f)\r\n",
    "test_data = [{'text': data['source']} for data in test_raw_data]\r\n",
    "label_map = {0: 'Positive', 1: 'Misspelled words', 2: 'Semantic Error'}\r\n",
    "\r\n",
    "results = predict(model, test_data, tokenizer, label_map, batch_size=32)\r\n",
    "for idx, text in enumerate(test_data):\r\n",
    "    resule_save.append({'source': text['text'], 'type': results[idx], 'id': test_raw_data[idx]['id']})\r\n",
    "    print('Data: {} \\t Lable: {}'.format(text, results[idx]))\r\n",
    "\r\n",
    "print(len(resule_save))\r\n",
    "with open('all_data/preliminary_a_test_source_with_type.json', 'w') as f:\r\n",
    "    json.dump(resule_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 然后定义ERNIE-CSC错别字纠正函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/ernie_csc\n",
      "/home/aistudio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-07-19 00:37:33,442] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-3.0-xbase-zh/ernie_3.0_xbase_zh_vocab.txt\n",
      "[2022-07-19 00:37:33,466] [    INFO] - tokenizer config file saved in /home/aistudio/.paddlenlp/models/ernie-3.0-xbase-zh/tokenizer_config.json\n",
      "[2022-07-19 00:37:33,468] [    INFO] - Special tokens file saved in /home/aistudio/.paddlenlp/models/ernie-3.0-xbase-zh/special_tokens_map.json\n",
      "W0719 00:37:34.985518 19333 analysis_predictor.cc:1086] The one-time configuration of analysis predictor failed, which may be due to native predictor called first and its configurations taken effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: 遇到逆竟时，我们必须勇于面对，而且要愈挫愈勇，这样我们才能朝著成功之路前进。\n",
      "Target: 遇到逆境时，我们必须勇于面对，而且要愈挫愈勇，这样我们才能朝著成功之路前进。\n",
      "Source: 人生就是如此，经过磨练才能让自己更加拙壮，才能使自己更加乐观。\n",
      "Target: 人生就是如此，经过磨练才能让自己更加茁壮，才能使自己更加乐观。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[35m--- Running analysis [ir_graph_build_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_graph_clean_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_analysis_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [is_test_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [simplify_with_basic_ops_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [embedding_eltwise_layernorm_fuse_pass]\u001b[0m\n",
      "I0719 00:37:36.247498 19333 fuse_pass_base.cc:57] ---  detected 1 subgraphs\n",
      "\u001b[32m--- Running IR pass [multihead_matmul_fuse_pass_v2]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass]\u001b[0m\n",
      "I0719 00:37:36.575968 19333 fuse_pass_base.cc:57] ---  detected 242 subgraphs\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass]\u001b[0m\n",
      "I0719 00:37:36.604575 19333 fuse_pass_base.cc:57] ---  detected 80 subgraphs\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [fc_fuse_pass]\u001b[0m\n",
      "I0719 00:37:37.505250 19333 fuse_pass_base.cc:57] ---  detected 242 subgraphs\n",
      "\u001b[32m--- Running IR pass [fc_elementwise_layernorm_fuse_pass]\u001b[0m\n",
      "I0719 00:37:37.763347 19333 fuse_pass_base.cc:57] ---  detected 80 subgraphs\n",
      "\u001b[32m--- Running IR pass [conv_elementwise_add_act_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_elementwise_add2_act_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_elementwise_add_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [transpose_flatten_concat_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [runtime_context_cache_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_params_sync_among_devices_pass]\u001b[0m\n",
      "I0719 00:37:37.906049 19333 ir_params_sync_among_devices_pass.cc:100] Sync params from CPU to GPU\n",
      "\u001b[1m\u001b[35m--- Running analysis [adjust_cudnn_workspace_size_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [inference_op_replace_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_graph_to_program_pass]\u001b[0m\n",
      "I0719 00:37:39.005333 19333 analysis_predictor.cc:1007] ======= optimize end =======\n",
      "I0719 00:37:39.032099 19333 naive_executor.cc:102] ---  skip [feed], feed -> pinyin_ids\n",
      "I0719 00:37:39.032125 19333 naive_executor.cc:102] ---  skip [feed], feed -> input_ids\n",
      "I0719 00:37:39.041869 19333 naive_executor.cc:102] ---  skip [softmax_21.tmp_0], fetch -> fetch\n",
      "I0719 00:37:39.041906 19333 naive_executor.cc:102] ---  skip [linear_364.tmp_1], fetch -> fetch\n",
      "W0719 00:37:39.045892 19333 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0719 00:37:39.049258 19333 gpu_context.cc:306] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "%cd ernie_csc/\r\n",
    "from paddlenlp.transformers import ErnieTokenizer\r\n",
    "from predict import Predictor\r\n",
    "from paddlenlp.data import Vocab\r\n",
    "%cd ../\r\n",
    "\r\n",
    "ernie_tokenizer = ErnieTokenizer.from_pretrained('ernie-3.0-xbase-zh')\r\n",
    "pinyin_vocab = Vocab.load_vocabulary('ernie_csc/pinyin_vocab.txt',\r\n",
    "                                     unk_token='[UNK]',\r\n",
    "                                     pad_token='[PAD]')\r\n",
    "ernie_predictor = Predictor('ernie_csc_infer_model/static_graph_params.pdmodel',\r\n",
    "                            'ernie_csc_infer_model/static_graph_params.pdiparams', \r\n",
    "                            'gpu', 128, ernie_tokenizer, pinyin_vocab)\r\n",
    "\r\n",
    "\r\n",
    "def ernie_predict(samples: list, batch_size=2) -> list:\r\n",
    "    results = ernie_predictor.predict(samples, batch_size=batch_size)\r\n",
    "    \r\n",
    "    return results\r\n",
    "\r\n",
    "\r\n",
    "# 使用示例\r\n",
    "# samples = [\r\n",
    "#     '遇到逆竟时，我们必须勇于面对，而且要愈挫愈勇，这样我们才能朝著成功之路前进。',\r\n",
    "#     '人生就是如此，经过磨练才能让自己更加拙壮，才能使自己更加乐观。',\r\n",
    "# ]\r\n",
    "# results = ernie_predict(samples)\r\n",
    "# for source, target in zip(samples, results):\r\n",
    "#     print(\"Source:\", source)\r\n",
    "#     print(\"Target:\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 再定义T5纠正函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-07-19 00:38:10,781] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/Langboat/mengzi-t5-base/spiece.model\n",
      "[2022-07-19 00:38:10,783] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/community/Langboat/mengzi-t5-base/added_tokens.json and saved to /home/aistudio/.paddlenlp/models/Langboat/mengzi-t5-base\n",
      "[2022-07-19 00:38:10,785] [    INFO] - Downloading added_tokens.json from https://bj.bcebos.com/paddlenlp/models/community/Langboat/mengzi-t5-base/added_tokens.json\n",
      "[2022-07-19 00:38:10,821] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/community/Langboat/mengzi-t5-base/special_tokens_map.json and saved to /home/aistudio/.paddlenlp/models/Langboat/mengzi-t5-base\n",
      "[2022-07-19 00:38:10,823] [    INFO] - Downloading special_tokens_map.json from https://bj.bcebos.com/paddlenlp/models/community/Langboat/mengzi-t5-base/special_tokens_map.json\n",
      "[2022-07-19 00:38:10,857] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/Langboat/mengzi-t5-base/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: 遇到逆竟时，我们必须勇于面对，而且要愈挫愈勇，这样我们才能朝著成功之路前进。\n",
      "Target: 遇到逆境时，我们必须勇于面对，而且要愈挫愈勇，这样我们才能朝著成功之路前进。\n",
      "Source: 人生就是如此，经过磨练才能让自己更加拙壮，才能使自己更加乐观。\n",
      "Target: 人生就是如此，经过磨练才能让自己更加茁壮，才能使自己更加乐观。\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "from paddlenlp.transformers import T5ForConditionalGeneration, T5Tokenizer\r\n",
    "\r\n",
    "T5_tokenizer = T5Tokenizer.from_pretrained('Langboat/mengzi-t5-base')\r\n",
    "T5_model = T5ForConditionalGeneration.from_pretrained('./t5_ft_ckpt/model_38000_best')\r\n",
    "\r\n",
    "def T5_predict(samples: list) -> list:\r\n",
    "    res_t5_pre = []\r\n",
    "    for text in samples:\r\n",
    "        inputs = T5_tokenizer(text)\r\n",
    "        inputs = {k:paddle.to_tensor([v]) for (k, v) in inputs.items()}\r\n",
    "        output = T5_model.generate(**inputs)\r\n",
    "        gen_text = T5_tokenizer.decode(list(output[0].numpy()[0]), skip_special_tokens=True)\r\n",
    "        gen_text = gen_text.replace(',', '，')\r\n",
    "        # 这里是补充生成文本不完整，可能会确实后半部分文本\r\n",
    "        gen_text.replace(gen_text[-3:], text[text.rfind(gen_text[-3:]):])\r\n",
    "        gen_text = gen_text if text.rfind(gen_text[-3:]) == -1 else gen_text + text[text.rfind(gen_text[-3:])+3:]\r\n",
    "        res_t5_pre.append(gen_text)\r\n",
    "    \r\n",
    "    return res_t5_pre\r\n",
    "\r\n",
    "\r\n",
    "# 使用示例\r\n",
    "samples = [\r\n",
    "    '遇到逆竟时，我们必须勇于面对，而且要愈挫愈勇，这样我们才能朝著成功之路前进。',\r\n",
    "    '人生就是如此，经过磨练才能让自己更加拙壮，才能使自己更加乐观。',\r\n",
    "]\r\n",
    "results = T5_predict(samples)\r\n",
    "for source, target in zip(samples, results):\r\n",
    "    print(\"Source:\", source)\r\n",
    "    print(\"Target:\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 定义去重函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_duplication(text: str):\r\n",
    "    length = len(text)\r\n",
    "    for i in range(length):\r\n",
    "        cp_range = min(i+1, length-i-1)\r\n",
    "        j = 0\r\n",
    "        flag = False\r\n",
    "        for j in range(cp_range):\r\n",
    "            if text[i-j:i+1] == text[i+1:i+1+j+1]:\r\n",
    "                flag = True\r\n",
    "                break\r\n",
    "        if flag:\r\n",
    "            return text.replace(text[i-j:i+1], '', 1)\r\n",
    "\r\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\r\n",
    "res = []\r\n",
    "\r\n",
    "with open('all_data/preliminary_a_test_source_with_type.json') as f:\r\n",
    "    raw_data = json.load(f)\r\n",
    "\r\n",
    "    # 这里为了简单，就没有把同一种模型的调用合并成一个list\r\n",
    "    for data in raw_data:\r\n",
    "        if data['type'] == 'Positive':\r\n",
    "            res.append({'inference': data['source'], 'id': data['id']})\r\n",
    "        elif data['type'] == 'Misspelled words':\r\n",
    "            csc_res = ernie_predict([data['source']])[0]\r\n",
    "            if csc_res == data['source']:\r\n",
    "                remove_res = remove_duplication(csc_res)\r\n",
    "                if remove_res == csc_res:\r\n",
    "                    t5_res = T5_predict([csc_res])[0]\r\n",
    "                    res.append({'inference': t5_res, 'id': data['id']})\r\n",
    "                else:\r\n",
    "                    res.append({'inference': remove_res, 'id': data['id']})\r\n",
    "            else:\r\n",
    "                res.append({'inference': csc_res, 'id': data['id']})\r\n",
    "        else:\r\n",
    "            remove_res = remove_duplication(data['source'])\r\n",
    "            if remove_res == csc_res:\r\n",
    "                t5_res = T5_predict([csc_res])[0]\r\n",
    "                res.append({'inference': t5_res, 'id': data['id']})\r\n",
    "            else:\r\n",
    "                res.append({'inference': remove_res, 'id': data['id']})\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019\n"
     ]
    }
   ],
   "source": [
    "print(len(res))\r\n",
    "with open('preliminary_a_test_inference.json', 'w') as f:\r\n",
    "    json.dump(res, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
